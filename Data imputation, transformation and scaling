# ========================== LOAD NECESSARY LIBRARIES ==========================
# Install these if needed:
# install.packages(c("dplyr", "openxlsx"))
# install.packages("missForest")

library(dplyr)
library(openxlsx)
library(missForest)  # For Random Forest Imputation

# ========================== USER INPUT SECTION ==========================
# 1) List all the Excel files you want to merge.
#    -> You can include 2, 3, 10... any number of files.
file_paths <- c(
  "/correctpath/file1.xlsx",
  "/correctpath/file2.xlsx",
  "/correctpath/file3.xlsx",
  "/correctpath/file4.xlsx",
  "/correctpath/file5.xlsx",
  "/correctpath/file6.xlsx"
  # Add more paths here, e.g. "/Users/path_to_file3.xlsx"
)

# 2) Set ppm tolerance for m/z grouping (Orbitrap 140k: 3–5 ppm)
ppm_tolerance <- 5

# 3) Choose imputation method: "minimal" or "random_forest"
#    - "minimal": MetaboAnalyst-style replacement with small value
#    - "random_forest": RF imputation, then MetaboAnalyst-style small-value fix
imputation_method <- "random_forest"  # or "random_forest"

# 4) Output file path
output_path <- paste0("/correctpath/filecombined_ppm_merged_", imputation_method, ".xlsx")

# ========================== LOAD DATA FILES ==========================

# Read all files into a list of data frames
dfs <- lapply(file_paths, function(fp) {
  df <- read.xlsx(fp)
  
  # Basic checks
  if (!"mz" %in% names(df)) {
    stop(paste("File", fp, "does not contain a column named 'mz'."))
  }
  
  # Ensure mz is numeric
  df$mz <- as.numeric(df$mz)
  
  return(df)
})

# ========================== PPM-BASED m/z GROUPING ==========================
# Goal:
#  - Take all m/z values across all files
#  - Group them into clusters where values are within ppm_tolerance
#  - Replace original m/z in each file with a "group m/z" (mean m/z of the group)
#  - Then merge by this grouped m/z

# 1) Collect all unique m/z values (excluding NAs)
all_mz <- sort(unique(unlist(lapply(dfs, function(df) df$mz))))
all_mz <- all_mz[!is.na(all_mz)]

if (length(all_mz) == 0) {
  stop("No valid m/z values found across input files.")
}

# 2) Assign group IDs
#    - Start from smallest m/z as group seed
#    - Keep adding m/z to current group if within ppm_tolerance of the seed
group_index <- integer(length(all_mz))
current_group <- 1
seed_mz <- all_mz[1]
group_index[1] <- current_group

for (i in 2:length(all_mz)) {
  mz_i <- all_mz[i]
  ppm_diff <- abs(mz_i - seed_mz) / seed_mz * 1e6
  
  if (ppm_diff <= ppm_tolerance) {
    # Same group
    group_index[i] <- current_group
  } else {
    # Start new group
    current_group <- current_group + 1
    seed_mz <- mz_i
    group_index[i] <- current_group
  }
}

# 3) For each group, compute representative m/z as the mean of values in that group
group_mz_mean <- tapply(all_mz, group_index, mean)

# 4) Create a lookup: original m/z -> grouped m/z (mean)
#    (names are original mz values, values are grouped mean mz)
grouped_mz_vector <- group_mz_mean[group_index]       # same length as all_mz
mz_map <- setNames(grouped_mz_vector, all_mz)         # named vector

# 5) Apply the grouping to each data frame
dfs_grouped <- lapply(dfs, function(df) {
  # Map each mz to its group mean; if mz is NA, keep NA
  df$mz <- mz_map[as.character(df$mz)]
  return(df)
})

# ========================== MERGE ALL DATA FRAMES BY GROUPED m/z ==========================

df_combined <- Reduce(function(x, y) full_join(x, y, by = "mz"), dfs_grouped)

# Identify intensity columns (all non-mz columns)
intensity_cols <- setdiff(names(df_combined), "mz")

# Convert intensity columns to numeric
df_combined[intensity_cols] <- lapply(df_combined[intensity_cols], function(x) {
  as.numeric(as.character(x))
})

# ========================== IMPUTATION + METABOANALYST-STYLE REPLACEMENT ==========================

# We will:
#  - Treat NAs, zeros and negative values as "missing"
#  - For "minimal": replace all missing with small positive value (min_positive / 10)
#  - For "random_forest": RF-impute missing; then ensure no zeros/negatives remain by
#    replacing any <= 0 with min_positive/10

imputation_method <- match.arg(imputation_method, choices = c("minimal", "random_forest"))

data_int <- as.data.frame(df_combined[intensity_cols])

if (imputation_method == "minimal") {
  # --- MINIMAL IMPUTATION (MetaboAnalyst-style) ---
  
  # Treat zeros and negative values as missing
  data_int[data_int <= 0] <- NA
  
  # Find smallest positive value in the whole intensity matrix
  min_positive <- min(data_int[data_int > 0], na.rm = TRUE)
  
  if (!is.finite(min_positive)) {
    stop("No positive values found in intensity data – cannot perform minimal imputation.")
  }
  
  replacement_value <- min_positive / 10
  
  # Replace all NA (original NA, zeros, negatives) with small positive value
  data_int[is.na(data_int)] <- replacement_value
  
  df_combined[intensity_cols] <- data_int
  
} else if (imputation_method == "random_forest") {
  # --- RANDOM FOREST IMPUTATION + MetaboAnalyst-style fix ---
  
  # Treat zeros and negative values as missing before RF
  data_int[data_int <= 0] <- NA
  
  # Random Forest imputation
  rf_imputed <- missForest(data_int)
  imputed <- rf_imputed$ximp
  
  # After RF, ensure no zeros or negatives remain.
  # Find smallest positive value
  min_positive <- min(imputed[imputed > 0], na.rm = TRUE)
  
  if (!is.finite(min_positive)) {
    stop("No positive values after random forest imputation – cannot apply MetaboAnalyst-style replacement.")
  }
  
  replacement_value <- min_positive / 10
  
  # Replace any remaining zeros/negatives with small positive value
  imputed[imputed <= 0] <- replacement_value
  
  df_combined[intensity_cols] <- imputed
}

# ========================== SAVE RESULT ==========================

write.xlsx(df_combined, output_path, rowNames = FALSE)
cat("Combined, ppm-merged, and", imputation_method, "imputed data saved to:\n", output_path, "\n")

# ============================================================================
# ====================== LOG10 TRANSFORMATION =========================
# ============================================================================

df_log10 <- df_combined

# Apply log10 to intensity columns only
df_log10[intensity_cols] <- lapply(df_log10[intensity_cols], function(x) log10(x))

# Replace Inf and NaN resulting from log10(0) or invalid cases
for (col in intensity_cols) {
  df_log10[[col]][is.infinite(df_log10[[col]]) | is.na(df_log10[[col]])] <- log10(replacement_value)
}

# Save log10-transformed data
log_output_path <- "/correctpath/combined_log10_transformed.xlsx"
write.xlsx(df_log10, log_output_path, rowNames = FALSE)
cat("Log10-transformed data saved to:", log_output_path, "\n")


# ============================================================================
# ============================ SCALING SECTION ================================
# ============================================================================

# Choose scaling method: "pareto", "autoscale", or "none"
scaling_method <- "pareto"   # <-- change here as needed

scale_data <- function(df, method, intensity_cols) {
  
  df_scaled <- df
  
  if (method == "pareto") {
    # Pareto scaling = (x - mean) / sqrt(sd)
    df_scaled[intensity_cols] <- scale(
      df[intensity_cols],
      center = TRUE,
      scale = sqrt(apply(df[intensity_cols], 2, sd, na.rm = TRUE))
    )
    
  } else if (method == "autoscale") {
    # Autoscaling = Z-score = (x - mean) / sd
    df_scaled[intensity_cols] <- scale(df[intensity_cols], center = TRUE, scale = TRUE)
    
  } else if (method == "none") {
    # No scaling applied
    df_scaled <- df
  } else {
    stop("Invalid scaling option. Choose 'pareto', 'autoscale', or 'none'.")
  }
  
  return(df_scaled)
}

# Apply scaling
df_scaled <- scale_data(df_log10, scaling_method, intensity_cols)

# Save scaled data
scaled_output_path <- paste0("/correctpath/combined_scaled_", scaling_method, ".xlsx")
write.xlsx(df_scaled, scaled_output_path, rowNames = FALSE)
cat("Scaled data (", scaling_method, ") saved to: ", scaled_output_path, "\n")


